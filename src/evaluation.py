from itertools import chain
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import os
from sklearn.metrics import roc_curve, auc, precision_recall_curve


def evaluate_model(predictions: object, dataset: dict, save_dir: str):
    """
    Evaluates the developed predictive model
    :param predictions: predictions generated by the model
    :param dataset: a dataset for testing
    :return: Nothing
    """
    test = dataset['test']
    save_dir = os.path.join(save_dir, 'build_evaluate_classifier')

    # extract the batches
    ground_truth = list(chain.from_iterable([bat[1] for bat in test]))
    predictions = list(chain.from_iterable(predictions))

    # convert to cpu
    ground_truth = [g.numpy().tolist() for g in ground_truth]
    predictions_proba = [p[0].numpy().tolist() for p in predictions]
    predictions = [(1 if prob > 0.5 else 0) for prob in predictions_proba]

    # plot the confusiom matrix
    plot_confusion_matrix(save_dir=save_dir, true_labels=ground_truth, predicted_labels=predictions)

    # plot roc curve
    plot_roc_curve(save_dir=save_dir, ground_truth=ground_truth, predictions=predictions_proba)

    # plot pr curve
    plot_precision_recall_curve(save_dir=save_dir, ground_truth=ground_truth, predictions=predictions_proba)

    # generate metrics around performance
    accuracy = accuracy_score(ground_truth, predictions)
    precision = precision_score(ground_truth, predictions)
    recall = recall_score(ground_truth, predictions)
    f1 = f1_score(ground_truth, predictions)

    print("Accuracy:", accuracy)
    print("Precision:", precision)
    print("Recall (Sensitivity):", recall)
    print("F1 Score:", f1)


def plot_confusion_matrix(save_dir: str, true_labels: list, predicted_labels: list):
    """
    Generates a confusion matrix of the predictions
    :param save_dir: save path
    :param true_labels: ground truth labels
    :param predicted_labels: predictions
    :return: Nothing
    """
    cm = confusion_matrix(true_labels, predicted_labels)

    # Plot confusion matrix using seaborn heatmap
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(true_labels),
                yticklabels=np.unique(true_labels))
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title('Confusion Matrix')
    plt.savefig(os.path.join(save_dir, 'confusion_matrix.png'))


def plot_roc_curve(save_dir: str, ground_truth: list, predictions: list):
    """
    Plots a ROC curve of the predictions
    :param save_dir: save path
    :param ground_truth: ground truths
    :param predictions: predictions
    :return: None
    """
    fpr, tpr, thresholds = roc_curve(ground_truth, predictions)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.savefig(os.path.join(save_dir, 'roc_curve.png'))


def plot_precision_recall_curve(save_dir: str, ground_truth: list, predictions: list):
    """
    plot a precision recall curve
    :param save_dir: save path
    :param ground_truth: ground truths
    :param predictions: predictions
    :return: None
    """
    precision, recall, thresholds = precision_recall_curve(ground_truth, predictions)
    pr_auc = auc(recall, precision)

    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, color='blue', lw=2, label='PR curve (area = %0.2f)' % pr_auc)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend(loc="lower left")
    plt.grid(True)
    plt.savefig(os.path.join(save_dir, 'pr_curve.png'))