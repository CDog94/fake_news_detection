from itertools import chain
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import os
from sklearn.metrics import roc_curve, auc, precision_recall_curve


def evaluate_model(predictions: dict, dataset: list, save_dir: str):
    """
    Evaluates the developed predictive model
    :param predictions: predictions generated by the model
    :param dataset: a dataset for testing
    :return: Nothing
    """
    save_dir = os.path.join(save_dir, 'build_evaluate_classifier')
    model_results = format_data_for_evaluation(predictions=predictions, dataset=dataset)

    # plot roc curve
    plot_roc_curve(save_dir=save_dir, dataset=model_results)

    # plot the confusiom matrix
    plot_confusion_matrix(save_dir=save_dir, dataset=model_results)

    # plot pr curve
    plot_precision_recall_curve(save_dir=save_dir, dataset=model_results)

    # generate metrics around performance
    accuracy = accuracy_score(ground_truth, predictions)
    precision = precision_score(ground_truth, predictions)
    recall = recall_score(ground_truth, predictions)
    f1 = f1_score(ground_truth, predictions)

    print("Accuracy:", accuracy)
    print("Precision:", precision)
    print("Recall (Sensitivity):", recall)
    print("F1 Score:", f1)


def plot_confusion_matrix(save_dir: str, dataset: list):
    """
    Generates a confusion matrix of the predictions
    :param save_dir: save path
    :param true_labels: ground truth labels
    :param predicted_labels: predictions
    :return: Nothing
    """
    cm = confusion_matrix(true_labels, predicted_labels)

    # Plot confusion matrix using seaborn heatmap
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(true_labels),
                yticklabels=np.unique(true_labels))
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title('Confusion Matrix')
    plt.savefig(os.path.join(save_dir, 'confusion_matrix.png'))


def plot_roc_curve(save_dir: str, dataset: list):
    """
    Plots a ROC curve of the predictions
    :param save_dir: save path
    :param dataset: contains all the neccesary model outputs to compute ROC curves
    :return: None
    """
    mean_fpr = np.linspace(0, 1, 100)
    tprs = []

    fig, ax = plt.subplots()

    # iterate over each fold
    for i, data in enumerate(dataset):
        # Compute ROC curve for current fold
        fpr, tpr, _ = roc_curve(data['ground_truth'], data['predictions'])

        # Interpolate the ROC curve at mean_fpr
        tpr_interp = np.interp(mean_fpr, fpr, tpr)
        tpr_interp[0] = 0.0
        tprs.append(tpr_interp)

        # Plot ROC curve for current fold
        ax.plot(fpr, tpr, lw=1, alpha=0.3, label=f'Fold {i + 1}')

    # Compute mean and standard deviation of true positive rates
    mean_tpr = np.mean(tprs, axis=0)
    std_tpr = np.std(tprs, axis=0)

    # Plot mean ROC curve
    ax.plot(mean_fpr, mean_tpr, color='b', label='Mean ROC', lw=2)

    # Plot standard deviation bands
    ax.fill_between(mean_fpr, mean_tpr - std_tpr, mean_tpr + std_tpr, color='grey', alpha=0.2,
                    label=r'$\pm$ 1 std. dev.')

    # Plot ROC curve for random classifier
    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Random')

    # Set axis labels and legend
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.set_title('ROC Curve')
    ax.legend(loc='lower right')

    # Show plot
    plt.show()
    plt.savefig(os.path.join(save_dir, 'roc_curve.png'))


def plot_precision_recall_curve(save_dir: str, dataset: list):
    """
    plot a precision recall curve
    :param save_dir: save path
    :param ground_truth: ground truths
    :param predictions: predictions
    :return: None
    """
    precision, recall, thresholds = precision_recall_curve(ground_truth, predictions)
    pr_auc = auc(recall, precision)

    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, color='blue', lw=2, label='PR curve (area = %0.2f)' % pr_auc)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend(loc="lower left")
    plt.grid(True)
    plt.savefig(os.path.join(save_dir, 'pr_curve.png'))

def format_data_for_evaluation(predictions: dict, dataset: list) -> list:
    """
    Formats data for evaluation such as generating CIs, means etc
    :param predictions: predictions generated by the model
    :param dataset: dataset for testing
    :return: dictionary with formatted data
    """
    formatted_data = []
    for pred, truth in zip(predictions.values(), dataset):
        # extract data which is presented as batches
        ground_truth_labels = list(chain.from_iterable([bat[1] for bat in truth['test']]))
        prediction_probabilities = list(chain.from_iterable(pred))

        # get the data from tensors
        ground_truth_labels = [g.numpy().tolist() for g in ground_truth_labels]
        predictions_probabilities = [p[0].numpy().tolist() for p in prediction_probabilities]
        binary_predictions = [(1 if prob > 0.5 else 0) for prob in predictions_probabilities]

        formatted_data.append({
            'ground_truth': ground_truth_labels,
            'predictions':predictions_probabilities,
            'binary_predictions':binary_predictions
        })

    return formatted_data